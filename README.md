# Decision Trees and Ensemble Methods Assignment

This repository contains implementations and demonstrations of classic decision trees and ensemble methods from scratch, along with examples using popular gradient boosting frameworks for classification, regression, and ranking tasks.

## Contents

### 1. **From-Scratch Implementations in Colab**
   - [Gradient Boosting Machine (GBM)](link-to-colab)
   - [Random Forest](link-to-colab)
   - [AdaBoost](link-to-colab)
   - [Decision Trees](link-to-colab)

### 2. **Framework-Based Showcases in Colab**
   - **Gradient Boosting Classifiers**:
     - [Gradient Boosting Classifiers Notebook](link-to-colab-classifiers)
   - **Gradient Boosting Regression Techniques**:
     - [Gradient Boosting Regression Notebook](link-to-colab-regression)
   - **Gradient Boosting Ranking Techniques**:
     - [Gradient Boosting Ranking Notebook](link-to-colab-ranking)

### 3. **Video Walkthrough**
   - [Watch the Walkthrough](link-to-video)

## Instructions to Run

1. Clone this repository:
   ```bash
   git clone https://github.com/Mohib1402/Decision-Trees-and-Ensemble-Methods-Assignment.git
   ```
2. Open the desired Colab notebook via the provided links.
3. Execute the cells sequentially to see the results.

## Key Features

- **From-Scratch Implementations**: Build and understand classic algorithms in detail.
- **Framework Comparisons**: Explore state-of-the-art gradient boosting frameworks for classification, regression, and ranking.
- **Metrics and Visualizations**: Evaluate models using relevant metrics (accuracy, NDCG, MAP, etc.) and visualize results for comparison.

## Results Summary

### Gradient Boosting Classifiers
Performance comparison across various frameworks (XGBoost, CatBoost, LightGBM, etc.) based on metrics like accuracy, precision, recall, and F1-score.

### Gradient Boosting Regression
Evaluate regression frameworks (XGBoost, CatBoost, LightGBM) using metrics like MAE, MSE, and RÂ² Score.

### Gradient Boosting Ranking
Ranking performance comparison (NDCG, MAP) for XGBoost, CatBoost, and LightGBM.

## References

- [GBM Methodology](https://docs.google.com/presentation/d/19j3wC-8_cz41CIm88F6kOFU8ys7zVcRfaBw6SImAeWc/edit#slide=id.ga2af525914_0_6955)
- [Random Forest from Scratch](https://github.com/veb-101/Machine-Learning-Algorithms/blob/master/Random%20Forest/random_forest.ipynb)
- [AdaBoost Implementation](https://github.com/veb-101/Machine-Learning-Algorithms/tree/master/Boosting%20-%20AdaBoost)
- [Decision Trees Guide](https://github.com/veb-101/Machine-Learning-Algorithms/tree/master/Decision%20Trees)
- [Hands-On ML](https://github.com/ageron/handson-ml3)
